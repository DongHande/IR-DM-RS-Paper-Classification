# ICLR 2022

## Causality


- **From Intervention to Domain Transportation: A Novel Perspective to Optimize Recommendation**；*Da Xu, Yuting Ye, Chuanwei Ruan, Evren Korpeoglu, Sushant Kumar, Kannan Achan*
  - Information retrieval, Learning theory, Causal inference, Missing data, Overlapping, Reweighting, Optimal transport
- **Invariant Causal Representation Learning for Out-of-Distribution Generalization**；*Chaochao Lu, Yuhuai Wu, José Miguel Hernández-Lobato, Bernhard Schölkopf*
  - Causal Representation Learning,Out-of-distribution
  - We propose invariant Causal Representation Learning (iCaRL), an approach that enables out-of-distribution (OOD) generalization in the nonlinear setting (i.e., nonlinear representations and nonlinear classifiers).
## Graph mining
- **Handling Distribution Shifts on Graphs: An Invariance Perspective**；*Qitian Wu, Hengrui Zhang, Junchi Yan, David Wipf*
  - Representation Learning on Graphs, Out-of-Distribution Generalization, Domain Shift, Graph Structure Learning, Invariant Models
- **Expressiveness and Approximation Properties of Graph Neural Networks**；*Floris Geerts, Juan L Reutter*
  - Graph Neural Networks, Colour Refinement, Weisfeiler-Leman, Separation Power, Universality
  - A general methodology for assessing the expressive and approximation power of GNNs is presented.
- **Understanding over-squashing and bottlenecks on graphs via curvature**；*Jake Topping, Francesco Di Giovanni, Benjamin Paul Chamberlain, Xiaowen Dong, Michael M. Bronstein*
  - Graph neural networks, Geometric deep learning, Differential geometry, Ricci curvature
- **Neural Structured Prediction for Inductive Node Classification**；*Meng Qu, Huiyu Cai, Jian Tang*
  - Graph neural networks
  - This paper studies node classification in the inductive setting, i.e., aiming to learn a model on labeled training graphs and generalize it to infer node labels on unlabeled test graphs. 
- **Data-Efficient Graph Grammar Learning for Molecular Generation**；*Minghao Guo, Veronika Thost, Beichen Li, Payel Das, Jie Chen, Wojciech Matusik*
  - molecular generation, graph grammar, data efficient generative model
- **A New Perspective on "How Graph Neural Networks Go Beyond Weisfeiler-Lehman?"**；*Asiri Wijesinghe, Qing Wang*
  - Graph Neural Networks, Graph Isomorphism, Weisfeiler Lehman
- **Topological Graph Neural Networks**；*Max Horn, Edward De Brouwer, Michael Moor, Yves Moreau, Bastian Rieck, Karsten Borgwardt*
  - topology, persistent homology, gnn, graph neural networks, graph classification, node classification, filtrations, topological data analysis, tda
  - We describe a new layer for graph neural networks that incorporates multi-scale (ranging from local to global) topological information.
- **Simple GNN Regularisation for 3D Molecular Property Prediction and Beyond**；*Jonathan Godwin, Michael Schaarschmidt, Alexander L Gaunt, Alvaro Sanchez-Gonzalez, Yulia Rubanova, Petar Veličković, James Kirkpatrick, Peter Battaglia*
  - Graph Neural Networks, GNNs, Deep Learning, Molecular Property Prediction
  - A simple regularisation technique for GNNs applied to 3D molecular property prediction & beyond.
- **Learning to Extend Molecular Scaffolds with Structural Motifs**；*Krzysztof Maziarz, Henry Richard Jackson-Flux, Pashmina Cameron, Finton Sirockin, Nadine Schneider, Nikolaus Stiefl, Marwin Segler, Marc Brockschmidt*
  - molecules, graph neural networks, scaffold, generative model
  - We propose a new fragment-based generative model of molecules that can be constrained to include an arbitrary subgraph (scaffold).
- **DEGREE: Decomposition Based Explanation for Graph Neural Networks**；*Qizhang Feng, Ninghao Liu, Fan Yang, Ruixiang Tang, Mengnan Du, Xia Hu*
  - XAI, GNN
  - We propose a new decomposition based explanation for Graph Neural Networks.



## Debiasing、fairness、long-tail、ODD、domain adaption、鲁棒性、降噪 （包括推荐和非推荐的都算）
- **Handling Distribution Shifts on Graphs: An Invariance Perspective**；*Qitian Wu, Hengrui Zhang, Junchi Yan, David Wipf*
  - Representation Learning on Graphs, Out-of-Distribution Generalization, Domain Shift, Graph Structure Learning, Invariant Models
- **From Intervention to Domain Transportation: A Novel Perspective to Optimize Recommendation**；*Da Xu, Yuting Ye, Chuanwei Ruan, Evren Korpeoglu, Sushant Kumar, Kannan Achan*
  - Information retrieval, Learning theory, Causal inference, Missing data, Overlapping, Reweighting, Optimal transport
- **Cross-Domain Imitation Learning via Optimal Transport**；*Junguang Jiang, Baixu Chen, Jianmin Wang, Mingsheng Long*
  - optimal transportation, imitation learning, cross-domain imitation learning, gromov-Wasserstein
  - We study the use of Gromov-Wasserstein for cross-domain imitation learning
- **Decoupled Adaptation for Cross-Domain Object Detection**；*Arnaud Fickinger, Samuel Cohen, Stuart Russell, Brandon Amos*
  - Object Detection, Domain Adaptation, Object Localization, Deep Learning, Transfer Learning
  - To deal with the challenges in cross-domain object detection, we propose D-adapt to decouple the adversarial adaptation and the training of the detector, and also decouple the category adaptation and the bounding box adaptation.
- **Fine-Tuning can Distort Pretrained Features and Underperform Out-of-Distribution**；*Ananya Kumar, Aditi Raghunathan, Robbie Matthew Jones, Tengyu Ma, Percy Liang*
  - fine-tuning theory, transfer learning theory, fine-tuning, distribution shift, implicit regularization
  - Fine-tuning does better than linear probing (training a linear classifier on pretrained features) in-distribution, but worse out-of-distribution (OOD)---we analyze why this happens and propose a way to get the benefits of both.
- **Asymmetry Learning for Counterfactually-invariant Classification in OOD Tasks**；*S Chandra Mouli, Bruno Ribeiro*
  - out-of-distribution classification, symmetries, counterfactual invariances, geometric deep learning
  - Counterfactual-invariant representations for symmetry transformations
- **Extending the WILDS Benchmark for Unsupervised Adaptation**；*Shiori Sagawa, Pang Wei Koh, Tony Lee, Irena Gao, Sang Michael Xie, Kendrick Shen, Ananya Kumar, Weihua Hu, Michihiro Yasunaga, Henrik Marklund, Sara Beery, Etienne David, Ian Stavness, Wei Guo, Jure Leskovec, Kate Saenko, Tatsunori Hashimoto, Sergey Levine, Chelsea Finn, Percy Liang*
  - distribution shifts, adaptation, unlabeled data
  - We introduce U-WILDS, which augments the WILDS distribution shift benchmark with realistic unlabeled data, and benchmark existing methods for unlabeled data on these in-the-wild distribution shifts.
- **Non-Transferable Learning: A New Approach for Model Ownership Verification and Applicability Authorization**；*Lixu Wang, Shichao Xu, Ruiqi Xu, Xiao Wang, Qi Zhu*
  - Domain Adaptation, Transfer Learning, Societal Considerations of Representation Learning, Model Watermark
  - We propose a novel Non-Transferable Learning (NTL) method to restrict the model generalization ability to certain domains for model ownership verification and applicability authorization.
- **A Fine-Grained Analysis on Distribution Shift**；*Olivia Wiles, Sven Gowal, Florian Stimberg, Sylvestre-Alvise Rebuffi, Ira Ktena, Krishnamurthy Dj Dvijotham, Ali Taylan Cemgil*
  - robustness, distribution shifts
  - We investigate and analyse the robustness of a variety of methods under distribution shifts using our flexible experimental framework.
- **Resolving Training Biases via Influence-based Data Relabeling**；*Shuming Kong, Yanyan Shen, Linpeng Huang*
  - Training bias, influence functions, data relabeling
  - We propose an influence-based relabeling framework for solving training bias with a theoretical guarantee.
- **Practical Integration via Separable Bijective Networks**；*Christopher M Bender, Patrick Emmanuel, Michael K. Reiter, Junier Oliva*
  - integration, flow, likelihood, classification, regression, out of distribution, regularization
  - We explore a method that enables learning over hypervolumes within the data space.






## 交互式推荐、Bandit、强化学习
- **HyAR: Addressing Discrete-Continuous Action Reinforcement Learning via Hybrid Action Representation**；*Boyan Li, Hongyao Tang, YAN ZHENG, Jianye HAO, Pengyi Li, Zhen Wang, Zhaopeng Meng, LI Wang*

  - Reinforcement Learning,Discrete-continuous hybrid action space
- **Know Your Action Set: Learning Action Relations for Reinforcement Learning**；*Ayush Jain, Norio Kosaka, Kyung-Min Kim, Joseph J Lim*
  - reinforcement learning, varying action space, relational reasoning
- **A Relational Intervention Approach for Unsupervised Dynamics Generalization in Model-Based Reinforcement Learning**；*Jiaxian Guo, Mingming Gong, Dacheng Tao*
  - Model-Based Reinforcement Learning, Unsupervised Dynamics Generalization
- **Topological Experience Replay**；*Zhang-Wei Hong ~Zhang-Wei_Hong1 , Tao Chen, Yen-Chen Lin, Joni Pajarinen, Pulkit Agrawal*
  - Deep reinforcement learning, experience replay
  - We rearrange the update order of experience for training the Q-function by a dependency graph.
- **The Boltzmann Policy Distribution: Accounting for Systematic Suboptimality in Human Models**；*Cassidy Laidlaw, Anca Dragan*
  - human model, boltzmann rationality, suboptimality, HRI, human-robot collaboration, generative models, reinforcement learning, deep RL
  - We propose modeling human behavior with a Boltzmann distribution over policies—not trajectories—and show it is more accurate and useful.
- **Value Function Spaces: Skill-Centric State Abstractions for Long-Horizon Reasoning**；*Dhruv Shah, Peng Xu, Yao Lu, Ted Xiao, Alexander T Toshev, Sergey Levine, brian ichter*
  - hierarchical reinforcement learning, planning, representation learning, robotics
  - We introduce value function spaces, a learned representation of state through the values of low-level skills, which capture affordances and ignores distractors to enable long-horizon reasoning and zero-shot generalization.
- **Generalisation in Lifelong Reinforcement Learning through Logical Composition**；*Geraud Nangue Tasse, Steven James, Benjamin Rosman*
   - Reinforcement Learning, Lifelong learning, Multi task learning, Transfer learning, Logical composition, Deep Reinforcement Learning
   - A framework with theoretical guarantees for an agent to quickly generalize over a task space by autonomously determining whether a new task can be solved zero-shot using existing skills, or whether a task-specific skill should be learned few-shot.
 - **Provably Filtering Exogenous Distractors using Multistep Inverse Dynamics**；*Yonathan Efroni, Dipendra Misra, Akshay Krishnamurthy, Alekh Agarwal, John Langford*
   - Reinforcement Learning Theory, Invariant Representation, Rich Observation Reinforcement Learning, Exogenous Noise, Inverse Dynamics
- **Transform2Act: Learning a Transform-and-Control Policy for Efficient Agent Design**；*Ye Yuan, Yuda Song, Zhengyi Luo, Wen Sun, Kris M. Kitani*
   - Agent Design, Morphology Optimization, Reinforcement Learning
 - **The Information Geometry of Unsupervised Reinforcement Learning**；*Benjamin Eysenbach, Ruslan Salakhutdinov, Sergey Levine*
   - unsupervised skill learning, reward-free RL, mutual information, DIAYN
   - We show that mutual information skill learning is optimal in one sense but not optimal in another sense.
 - **Accelerated Policy Learning with Parallel Differentiable Simulation**；*Jie Xu, Viktor Makoviychuk, Yashraj Narang, Fabio Ramos, Wojciech Matusik, Animesh Garg, Miles Macklin*
   - Robot Control, Policy Learning, Differentiable Simulation, Reinforcement Learning
   - We propose an efficient policy learning method leveraging the recent advance of differentiable simulation, and our method outperforms state-of-the-art algorithms in both sample efficiency and wall clock time on multiple challenging control tasks.
 - **Model-Based Offline Meta-Reinforcement Learning with Regularization**；*Sen Lin, Jialin Wan, Tengyu Xu, Yingbin Liang, Junshan Zhang*
   - offline reinforcement learning, model-based reinforcement learning, behavior policy, Meta-reinforcement learning
   - This paper proposes a novel offline Meta-RL algorithm with regularization, which has provable performance improvement and outperforms the existing baselines empirically.
 - **Learning State Representations via Retracing in Reinforcement Learning**；*Changmin Yu, Dong Li, Jianye HAO, Jun Wang, Neil Burgess*
   - Representation learning, model-based reinforcement learning
   - We introduce Learning via Retracing, a novel self-supervised framework based on temporal cycle-consistency assumption of the transition dynamics, for improved learning of the representation (and the dynamics model) in RL tasks.
 - **LIGS: Learnable Intrinsic-Reward Generation Selection for Multi-Agent Learning**；*David Henry Mguni, Taher Jafferjee, Jianhong Wang, Nicolas Perez-Nieves, Oliver Slumbers, Feifei Tong, Yang Li, Jiangcheng Zhu, Yaodong Yang, Jun Wang*
   - multi-agent, reinforcement learning, intrinsic rewards, exploration
 - **Efficient Active Search for Combinatorial Optimization Problems**；*André Hottung, Yeong-Dae Kwon, Kevin Tierney*
   - heuristic search, combinatorial optimization, learning to optimize, reinforcement learning, traveling salesperson problem, vehicle routing problem, job shop scheduling problem
   - We propose active search approaches for combinatorial optimization problems that search for solutions by adjusting a subset of (model) parameters to a single instance at test time.



 

## 其他方面的推荐
- **Bridging Recommendation and Marketing via Recurrent Intensity Modeling**；*Yifei Ma, Ge Liu, Anoop Deoras*
  - Recommender systems, marketing, push notifications, temporal point processes, sequence models
  - 我们将项目推荐系统的目的重新定义为向项目提供者推荐用户，以达到内容推广和多样性的目的。

## 搜索
- **ARTEMIS: Attention-based Retrieval with Text-Explicit Matching and Implicit Similarity**；*Ginger Delmas, Rafael S. Rezende, Gabriela Csurka, Diane Larlus*
  - we exploit the specific relation of each query element with the targeted image and derive light-weight attention mechanisms which enable to mediate between the two complementary modalities. 


## 模型可解释性
- **FastSHAP: Real-Time Shapley Value Estimation**；*Neil Jethani, Mukund Sudarshan, Ian Connick Covert, Su-In Lee, Rajesh Ranganath*
  - interpretability, shapley, amortization, explainability, game theory
  - We introduce FastSHAP, a new method for estimating Shapley values in a single forward pass using an explainer model that is learned via stochastic gradient optimization using a weighted least squares-like objective function.
- **DISCOVERING AND EXPLAINING THE REPRESENTATION BOTTLENECK OF DNNS**；*Huiqi Deng, Qihan Ren, Hao Zhang, Quanshi Zhang*
  - representation bottleneck, representation ability, interaction, explanation
- **DISSECT: Disentangled Simultaneous Explanations via Concept Traversals**；*Asma Ghandeharioun, Been Kim, Chun-Liang Li, Brendan Jou, Brian Eoff, Rosalind Picard*
  - Explainability, Interpretability, Counterfactual generation, Generative Adversarial Network, Variational Autoencoder
  - We propose a novel counterfactual explainability method that simultaneously satisfies several desirable qualities where other methods fail by training a generator, a discriminator, and a concept disentangler using the classifier’s signal.
- **Fooling Explanations in Text Classifiers**；*Adam Ivankay, Ivan Girardi, Chiara Marchiori, Pascal Frossard*
  - robustness, explainability, text classification, natural language processing
  - Our work shows that explanation methods in text classifiers are susceptible to imperceptible perturbations that alter the explanation outcomes without changing the predictions of the classifiers.



## 其他
- **Understanding the Variance Collapse of SVGD in High Dimensions**；*Jimmy Ba, Murat A Erdogdu, Marzyeh Ghassemi, Shengyang Sun, Taiji Suzuki, Denny Wu, Tianzong Zhang*

  - Stein Variational Gradient Descent,Approximate Inference, Particle-based Variational Inference
- **Discriminative Similarity for Data Clustering**；*Yingzhen Yang, Ping Li*
  - Discriminative Similarity, Rademacher Complexity, Generalization Bound, Data Clustering
- **Generalization of Neural Combinatorial Solvers Through the Lens of Adversarial Robustness**；*Simon Geisler, Johanna Sommer, Jan Schuchardt, Aleksandar Bojchevski, Stephan Günnemann*
  - Generalization, Neural Combinatorial Optimization, Adversarial Robustness
- **Constructing Orthogonal Convolutions in an Explicit Manner**；*Tan Yu, Jun Li, YUNFENG CAI, Ping Li*
  - orthogonal convolution
- **Concurrent Adversarial Learning for Large-Batch Training**；*Yong Liu, Xiangning Chen, Minhao Cheng, Cho-Jui Hsieh, Yang Youi*
  - Distributed Machine Learnig, Large-Batch Training, Adversarial Learning
- **Transformer Embeddings of Irregularly Spaced Events and Their Participants**；*Hongyuan Mei, Chenghao Yang, Jason Eisner*
  - irregular time series, generative Transformers, neuro-symbolic architectures, logic programming
- **Self-Joint Supervised Learnin**；*Navid Kardan ~Navid_Kardan1 , Mubarak Shah, Mitch Hill*
  - Supervised learning， i.i.d. assumption，
  - 模型明确地学习了条件独立性的样本与样本之间的关系，而不是假设样本是独立的。
- **Minimax Optimization with Smooth Algorithmic Adversaries**；*Tanner Fiez, Chi Jin, Praneeth Netrapalli, Lillian J Ratliff*
  - Minimax optimization, two player zero sum games, generative adversarial networks, adversarial training
- **Dual Lottery Ticket Hypothesis**；*Yue Bai, Huan Wang, ZHIQIANG TAO, Kunpeng Li, Yun Fuf*
  - Dual Lottery Ticket Hypothesis, Sparse Network Training
  - 我们提出了一个双彩票假设(DLTH)和一个训练随机稀疏网络策略来验证DLTH。
- **Incremental False Negative Detection for Contrastive Learning**；*Tsai-Shien Chen, Wei-Chih Hung, Hung-Yu Tseng, Shao-Yi Chien, Ming-Hsuan Yang*
  - Self-supervised learning, Contrastive learning, Representation learning, Clustering-based learning
  - 本文探讨了假阴性样本在自我监督对比学习中的作用，并引入了一个增量检测和显式删除假阴性样本的框架
- **On the Convergence of Certified Robust Training with Interval Bound Propagation**；*Yihan Wang, Zhouxing Shi, Quanquan Gu, Cho-Jui Hsieh*
  - Certified robustness, Adversarial robustness, Convergence
  - We present the first theoretical analysis on the convergence of certified robust training with interval bound propagation.
- **Particle Stochastic Dual Coordinate Ascent: Exponential convergent algorithm for mean field neural network optimization**；*Kazusato Oko, Taiji Suzuki, Atsushi Nitanda, Denny Wu*
  - Neural Network Optimization, Mean field Regime, Overparameterization
  - Proposed a new algorithm for optimizing two-layer neural network in the mean field regime that achieves exponential convergence in regularized empirical risk minimization (w.r.t. outer loop iterations).
- **How many degrees of freedom do we need to train deep networks: a loss landscape perspective**；*Brett W Larsen ~Brett_W_Larsen1 , Stanislav Fort, Nic Becker, Surya Ganguli*
  - loss landscape, high-dimensional geometry, random hyperplanes, optimization
- **Monotonic Differentiable Sorting Networks**；*Felix Petersen, Christian Borgelt, Hilde Kuehne, Oliver Deussen*
  - differentiable sorting, monotonic, sorting, ranking, sorting networks
 - **Comparing Distributions by Measuring Differences that Affect Decision Making**；*Shengjia Zhao, Abhishek Sinha, Yutong He, Aidan Perreault, Jiaming Song, Stefano Ermon*
    - probability divergence, two sample test, generative model
    - 这篇文章提出了一种新的不同数据分布差异性的测量方法
- **Bootstrapped Meta-Learning**；*Sebastian Flennerhag, Yannick Schroecker, Tom Zahavy, Hado van Hasselt, David Silver, Satinder Singh*
  - meta-learning, meta-gradients, meta-reinforcement learning
  - We propose an algorithm for meta-learning with gradients that bootstraps the meta-learner from itself or another update rule. 
- **Minibatch vs Local SGD with Shuffling: Tight Convergence Bounds and Beyond**；*Chulhee Yun, Shashank Rajput, Suvrit Sra*
  - Local SGD, Minibatch SGD, Shuffling, Without-replacement, Convex Optimization, Stochastic Optimization, Federated Learning, Large Scale Learning, Distributed Learning
  - We provide tight upper and lower bounds on convergence rates of shuffling-based minibatch SGD and local SGD, and propose an algorithmic modification that improves convergence rates beyond our lower bounds.
- **The Hidden Convex Optimization Landscape of Regularized Two-Layer ReLU Networks: an Exact Characterization of Optimal Solutions**；*Yifei Wang, Jonathan Lacotte, Mert Pilanci*
  - Neural networks, global optimization, convex optimization, convex analysis
  - We prove that finding all globally optimal two-layer ReLU neural networks can be performed by solving a convex optimization program with cone constraints. 
- **Meta-Learning with Fewer Tasks through Task Interpolation**；*Huaxiu Yao, Linjun Zhang, Chelsea Finn*
  - meta-learning, task interpolation, meta-regularization
  - A new framework to densify the task distribution via task interpolation. 
- **Hyperparameter Tuning with Renyi Differential Privacy**；*Nicolas Papernot, Thomas Steinke*
  - differential privacy, hyperparameter tuning
  - We provide privacy guarantees for hyperparameter search procedures, showing that tuning hyperparameters leaks private information, but that, under certain assumptions, this leakage is modest. 
- **Robust Unlearnable Examples: Protecting Data Privacy Against Adversarial Learning**；*Shaopeng Fu, Fengxiang He, Yang Liu, Li Shen, Dacheng Tao*
  - unlearnable examples, adversarial training, privacy
  - This paper proposes an robust error-minimizing noise that can protect data from being learned under adversarial training.
- **Demystifying Batch Normalization in ReLU Networks: Equivalent Convex Optimization Models and Implicit Regularization**；*Tolga Ergen, Arda Sahiner, Batu Ozturkler, John M. Pauly, Morteza Mardani, Mert Pilanci*
  - batch normalization, ReLU networks, deep networks, convex optimization, whitening, implicit regularization, algorithmic bias
  - We introduce an analytic framework based on convex duality to obtain exact and polynomial-time trainable convex representations of weight-decay regularized ReLU networks with BN.
- **Prospect Pruning: Finding Trainable Weights at Initialization using Meta-Gradients**；*Milad Alizadeh, Shyam A. Tailor, Luisa M Zintgraf, Joost van Amersfoort, Sebastian Farquhar, Nicholas Donald Lane, Yarin Gal*
  - pruning, lottery ticket hypothesis, pruning at initialization
  - We use meta-gradients to prune neural networks at initialization based on "trainability" of weights instead of their impact on the loss at a single step.
- **On the Convergence of mSGD and AdaGrad for Stochastic Optimization**；*ruinan Jin, Yu Xing, Xingkang He*
  - stochastic gradient descent, adaptive gradient algorithm, asymptotic convergence
  - A theoretical paper focusing on the investigation for the convergence of mSGD and AdaGrad optimization algorithms.
- **Continual Normalization: Rethinking Batch Normalization for Online Continual Learning**；*Quang Pham, Chenghao Liu, Steven HOI*
  - ontinual Learning, Batch Normalization
  - A negative effect of BN in online continual learning and a simple strategy to alleviate it.




